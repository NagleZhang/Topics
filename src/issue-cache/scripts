# 问题描述
        事情是这样的, 我们收到很多用户投诉, 说是有很多系统卡死的情况.
        经过调查日志发现, 数据库当中, 查询从来没有的超过 1000ms 的查询增加了很多
        在监控系统当中, 我们有看到了这些数据.可以看到, 红色线条代表的cpu 基本上稳定在 50% 左右,
        黄色的内存的利用率差不多在70到80%
        而磁盘的 IO 利用率, 则有非常明显的升高, 从 50% , 增加到 85%
        而且, 与1000ms以上的查询发生时间一致.
        从这些条件来看, 问题在磁盘 IO 的概率很大.
        但是, 从这些条件来看, 锅还不能明显断定就是我们的.
        我们需要进一步的断定.
        登陆到了机器上面, 以秒为单位来计算IO的利用率.我们得到以下的利用率情况.
        (一张 60% - 100% 的变化图):
        可以看到,当以更细粒度的在电脑上看一个利用率的时候, 利用率就从原有的80% 变成了从 60 到100之间上下浮动.
        这是因为我们监控系统当中聚合的时间是 5s, 而这中间, 100 与 60 被5s平均以后,就变成了 %80.
        所以,系统当中出现的个别情况,就是正好磁盘IO利用率达到了100%,并且数据库在发生查询的时候发生的.

# 了解情况
        # 那么,问题确实是在磁盘io利用率上(开始背锅图片),接下来的问题是,
        # 为什么会发生磁盘利用率 100% 的情况?
        # 这里,我们来了解一下我们的文件系统基本结构.
        # 左边是我们的内存, 它被分为了一个一个的页面被进行操作.
        # 内存之上运行的是我们的文件系统,可以拥有路径, 文件, 然后用目录树的形式来管理
        # 每一个文件, 都会拥有它的 inode 结构. 它用于存储文件与实际数据的比如说大小,权限等等相关信息.
        # 右侧是我们的磁盘, 它拥有一个又一个的扇区.
        # 这些扇区被组织成了其上面的一种形式, 在地址的最开始会有 inode map,
        # 当程序根据 inode 来访问数据的时候, 首先会根据 inode 来获取其在磁盘当中的更具体的位置.
        # 然后再进行数据传输. 每一次的传输都是以 block 为单位.一般是512k
        # 在传统的硬盘当中, 依靠旋转来获取数据.
        # 我们所说的这个问题当中, 就是在内存和磁盘之间的IO利用率, 在瞬时达到了 100%.

# 问题分析
        # 一般来说, 磁盘IO大了, 要么是系统请求增加
        # 要么就是磁盘容量不够,从而以碎片的形式存储,进而导致寻址增加.从而增加 IO,提高了利用率.
        # 与人员沟通, 发现最近请求的数量并没有明显的增加.
        # 然后通过 df 命令查看文件系统的占用空间, 发现仅仅是 30%
        # emmmm(背景音即可)
        # 难不成是 cache ? 
        # 我们来看看 cache 的使用情况.
        # 使用 cachestat.py , 获取 cache 的 hit, 发现其数值是 91%, 并不低. cache 的size 是500mi.
        # 我们在另外一台机器取出一个进行对比, 发现其命中率是 97%. 而 cache 的 size 竟然是 300mi
        # cache 命中率降低, 并且cache 内存占用更高.
        # 不对劲.

# 进一步了解
        # 当文件系统访问磁盘的时候, 为了因为磁盘的访问效率相对于内存更低. 所以文件系统会在内存当中使用cache.(manim 文件系统变化的动画)
        # 而如果 cache 被占满, 则文件系统获取数据的时候, 无法从系统cache中获取, 就只能去磁盘当中拿. 这样就会导致io利用率升高.
        # 基本上可以断定就是 cache 的问题了.

# 问题结论及解决
        # 那么我们继续对比, 发现该机器某个的进程, 一直在持续消耗新的内存.而这部分内存, 消耗掉了本来文件系统cache可以占用的空间.
        # 从而导致 cache 命中率下降, 进而导致了 io 利用率的升高.
        # 于是 # kill $(pidof mihayo)
        # 我们的问题消失了.
